{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90351160-0cd4-46c3-816c-1bf932ff720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.8/dist-packages (0.23.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (2024.5.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560c7fc2-ed11-4a3a-bf95-dd36c6dd35a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import textwrap\n",
    "import os\n",
    "#####################\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "import torch\n",
    "from docx import Document\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from softwaredevelopmentapi.model_loader import llm_model,tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def load_all_models():\n",
    "\n",
    "    # Clears cache if you're using CUDA\n",
    "\n",
    "    global llm_model,tokenizer\n",
    "    #global img_to_text_pipe\n",
    "    # Load your model here, e.g., Hugging Face model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=True, device=device)\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    #model_name=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16,use_auth_token=\"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\")\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "class SoftwareDeveloperAgent:\n",
    "    def __init__(self, model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True,\n",
    "        #                                               use_auth_token=\"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\")\n",
    "        #self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16,\n",
    "        #                                                  use_auth_token=\"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\")\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True,\n",
    "                                  use_auth_token=\"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\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16,use_auth_token=\"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\")\n",
    "        terminators = [\n",
    "                        tokenizer.eos_token_id,\n",
    "                        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "                    ]\n",
    "    \n",
    "    def read_pdf(self, file_path):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        data = loader.load()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def read_docx(self, file_path):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        data = loader.load()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def read_txt(self, file_path):\n",
    "        loader = TextLoader(file_path)\n",
    "        data = loader.load()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def add_text_to_docx(self, file_path, text, output_path):\n",
    "        doc = Document(file_path)\n",
    "        doc.add_paragraph(text)\n",
    "        doc.save(output_path)\n",
    "\n",
    "    def generate_response_from_document(self, file_path, query, project_info):\n",
    "        #if len(project_info) > 0:\n",
    "        #    self.add_text_to_docx(file_path, project_info, \"Knowldgebase/temp/temp.docx\")\n",
    "        # \"\"\"Generate a response based on a user query from a document.\"\"\"\n",
    "        # Determine the file type and extract text\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            document_text = self.read_pdf(file_path)\n",
    "        elif file_path.lower().endswith('.docx'):\n",
    "            document_text = self.read_docx(file_path)\n",
    "        elif file_path.lower().endswith('.txt'):\n",
    "            document_text = self.read_txt(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Supported types are: 'pdf', 'docx', 'txt'.\")\n",
    "\n",
    "        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=2000,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "\n",
    "        # document_text[0]= f\"\\n{document_text[0]=}\\n{project_info}\"\n",
    "\n",
    "        llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0.7})\n",
    "\n",
    "        # Step 2: Split the text into manageable chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "        texts = text_splitter.split_documents(document_text)\n",
    "\n",
    "        # Step 3: Generate embeddings for the texts\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cuda\"}\n",
    "        )\n",
    "        db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "        SYSTEM_PROMPT = f\"\"\"\n",
    "       You are a knowledgeable AI Assistant, responsible for providing code to query . Your responses should be thorough, clear, and aligned with standard object oriented Programming Django python code standards. Always provide Django python code syntax only, while ensuring the code is accurate and relevant to provided project in the document.\n",
    "Always clarify the context if needed. {query}\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=\"\"\"You are a highly knowledgeable Chatbot, dedicated to providing accurate and detailed informative answers to user queries. Please don't generate irrelevant or extra information which the user did not ask you to generate.    \n",
    "Context: {context}\n",
    "User: {question}\n",
    "Chatbot:\"\"\", input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "        )\n",
    "\n",
    "        # Execute a query (you can customize this part as needed)\n",
    "        query_result = qa_chain(query)\n",
    "\n",
    "        # Execute the query\n",
    "        #         query_result, source_documents = qa_chain.run(query)\n",
    "\n",
    "        # print(\"llm!!!!!!!!!!!!!!!!!!!!! Result\",query_result)\n",
    "\n",
    "        # Extract the answer from the 'response' field using string manipulation\n",
    "        # Assuming the answer always follows \"Answer:\" and ends at the end of the string\n",
    "        response_content = query_result['result']\n",
    "        answer_prefix = \"Chatbot:\"\n",
    "        answer_start_index = response_content.find(answer_prefix)\n",
    "        if answer_start_index != -1:\n",
    "            answer = response_content[answer_start_index + len(answer_prefix):].strip()\n",
    "            print(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            print(\"No answer found in the response.\")\n",
    "            return response_content\n",
    "\n",
    "    def software_developer_agent_template(self, file_path, output_path, base_dir,projectid,ui_data):\n",
    "        \n",
    "        query= \"Generate the Clothing detailed  website HTML code according to provided sample html in document, Ensure that the generated code strictly adheres to the information provided without adding any extra content or features not mentioned in the document. Provide detailed and clear answers, focusing solely on the specified requirements. you should mention html file name. you should nav should have link of other linked pages to that html, should add web based images as example.\"\n",
    "        path=file_path\n",
    "#        sections_queries = {\n",
    "#          \"html\": { \n",
    "#              \"query\": f\"Write static html single page website with (.html files,style.css,script.js) code for the project based on the provided document with detailed answers. Do not generate any extra information which the user didn't ask for.\",\n",
    "#            \"path\": file_path}}\n",
    "        project_info = {}\n",
    "#        project_details = \"\"\n",
    "#        for section, details in sections_queries.items():\n",
    "        print(\"query\", query)\n",
    "        #query = f\"For Project {projectname}, {details.get('query')}\"\n",
    "        response = self.generate_response_from_document(path, query, project_info)\n",
    "        #if section == \"Django Directory\":\n",
    "        #    self.create_directory_structure_from_text(response, base_dir)\n",
    "        # Ensure the section key is formatted correctly\n",
    "        section_key = \"index\"\n",
    "        #project_info[section_key] = response\n",
    "        # Create a new .docx document\n",
    "        doc = Document()\n",
    "        doc.add_heading(section_key, 0)\n",
    "        doc.add_paragraph(response)\n",
    "        path = f'Knowldgebase/SD/{section_key}{projectid}.docx'\n",
    "        # Save the document\n",
    "        doc.save(path)\n",
    "        # Append the response to the project_info with a newline\n",
    "        #if project_details:\n",
    "        #    project_details += f\"\\n{section_key}:\\n{response}\"\n",
    "        #else:\n",
    "        #    project_details = f\"{section_key}:\\n{response}\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.reset_max_memory_cached()\n",
    "        # Free all unused cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.reset_max_memory_cached()\n",
    "        # Free all unused cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "        # Create a new .docx document\n",
    "        \n",
    "        #doc = Document()\n",
    "        #doc.add_heading('Static Website', 0)\n",
    "\n",
    "        #for section in sections_queries.keys():\n",
    "        #doc.add_heading(section.capitalize(), level=1)\n",
    "        #doc.add_paragraph(project_info[section.replace(\", \", \"_\").replace(\" \", \"_\")])\n",
    "\n",
    "        # Save the document\n",
    "        doc.save(output_path)\n",
    "        #return f\"SRS document saved to {output_path}\"\n",
    "        query= \"Generate the detailed static website styles.css code for each html provided in given document. this is the ui requirments.Ensure that the generated code strictly adheres to the information provided without adding any extra content or features not mentioned in the document. Provide detailed and clear answers, focusing solely on the specified requirements.\"\n",
    "        path=output_path\n",
    "#        sections_queries = {\n",
    "#          \"html\": { \n",
    "#              \"query\": f\"Write static html single page website with (.html files,style.css,script.js) code for the project based on the provided document with detailed answers. Do not generate any extra information which the user didn't ask for.\",\n",
    "#            \"path\": file_path}}\n",
    "        project_info = {}\n",
    "#        project_details = \"\"\n",
    "#        for section, details in sections_queries.items():\n",
    "        print(\"query\", query)\n",
    "        #query = f\"For Project {projectname}, {details.get('query')}\"\n",
    "        response = self.generate_response_from_document(path, query, project_info)\n",
    "        #if section == \"Django Directory\":\n",
    "        #    self.create_directory_structure_from_text(response, base_dir)\n",
    "        # Ensure the section key is formatted correctly\n",
    "        section_key = \"styles\"\n",
    "        #project_info[section_key] = response\n",
    "        # Create a new .docx document\n",
    "        doc = Document()\n",
    "        doc.add_heading(section_key, 0)\n",
    "        doc.add_paragraph(response)\n",
    "        path = f'Knowldgebase/SD/{section_key}_{projectid}.docx'\n",
    "        # Save the document\n",
    "        doc.save(path)\n",
    "        # Append the response to the project_info with a newline\n",
    "        #if project_details:\n",
    "        #    project_details += f\"\\n{section_key}:\\n{response}\"\n",
    "        #else:\n",
    "        #    project_details = f\"{section_key}:\\n{response}\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.reset_max_memory_cached()\n",
    "        # Free all unused cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.reset_max_memory_cached()\n",
    "        # Free all unused cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "        # Create a new .docx document\n",
    "        \n",
    "        #doc = Document()\n",
    "        #doc.add_heading('Static Website', 0)\n",
    "\n",
    "        #for section in sections_queries.keys():\n",
    "        #doc.add_heading(section.capitalize(), level=1)\n",
    "        #doc.add_paragraph(project_info[section.replace(\", \", \"_\").replace(\" \", \"_\")])\n",
    "\n",
    "        # Save the document\n",
    "        doc.save(path)\n",
    "        return \"Code has been generated.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371af2d4-34ec-46c5-8c68-c71282f37c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SoftwareDeveloperAgent()\n",
    "#projectname=\"Online Ecommerce Website on Django Platform\"\n",
    "output_path = 'htmloutput.docx'\n",
    "base_dir= 'outputhtml'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "input_path='Knowldgebase/SA/SRS{projectid}.docx'\n",
    "#response = agent.software_developer_agent_full(input_path, output_path,base_dir,projectid)\n",
    "response = agent.software_developer_agent_template(input_path, output_path, base_dir,projectid,ui_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
